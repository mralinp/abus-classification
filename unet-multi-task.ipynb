{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import unet\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(256, 256),\n",
    "            A.Rotate(limit=35, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Normalize(\n",
    "                mean=[0.0],\n",
    "                std=[1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "validation_transforms = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.Normalize(\n",
    "                mean=[0.0],\n",
    "                std=[1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.tdsc_2d.TDSC_2D(path=\"./data/tdsc/slices\", train=True, transforms=train_transforms)\n",
    "validation_dataset = datasets.tdsc_2d.TDSC_2D(path=\"./data/tdsc/slices\", train=False, transforms=validation_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256]) torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "x,y,l = train_dataset[0]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABUSClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, device = \"cpu\"):\n",
    "        super(ABUSClassifier, self).__init__()\n",
    "        self.device = device\n",
    "        # Base Model\n",
    "        self.base_model = unet.UNet(in_channels=1, out_channels=1)\n",
    "         # classification\n",
    "        self.slice_classification_block = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(1024*16*16, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.slice_classifier = torch.nn.Linear(256, 1)\n",
    "        # self.volumeClassifier = VolumeClassifier()\n",
    "        # self.slices_features = torch.tensor([]).to(device)\n",
    "    \n",
    "    # output cls_prediction for slices, segmentation predictions and cls_prediction for volume\n",
    "    def forward(self, x):\n",
    "        segmentation_pred = self.base_model(x)\n",
    "        bottle_neck_features = self.base_model.get_bottleneck_output()\n",
    "        cls_features = self.slice_classification_block(bottle_neck_features)\n",
    "        cls_predictions = self.slice_classifier(cls_features)\n",
    "        \n",
    "        return segmentation_pred, cls_predictions\n",
    "      \n",
    "    # knows that the volume mini-batches are finished and now it can classify then whole volume\n",
    "    # def end_volume(self):\n",
    "    #     ret = self.volumeClassifier(self.slices_features)\n",
    "    #     self.slices_features = torch.tensor([]).to(self.device)\n",
    "    #     return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256]) torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozma/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ABUSClassifier()\n",
    "x = torch.rand([1, 1, 256,256])\n",
    "print(x.shape)\n",
    "seg, cls = model(x)\n",
    "print(seg.shape, cls.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def init(self):\n",
    "        super(DiceLoss, self).init()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "       smooth = 1.\n",
    "       iflat = pred.contiguous().view(-1)\n",
    "       tflat = target.contiguous().view(-1)\n",
    "       intersection = (iflat * tflat).sum()\n",
    "       A_sum = torch.sum(iflat * iflat)\n",
    "       B_sum = torch.sum(tflat * tflat)\n",
    "       return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_epochs = 10\n",
    "alpha = 0.3 # how much slice classification is important for us\n",
    "beta = 0.2 # how much slice segmentation is important for us\n",
    "gamma = 0.5 # how much volume classification is important for us\n",
    "criterion_bce = torch.nn.BCEWithLogitsLoss()\n",
    "criterion_dice = DiceLoss()\n",
    "model = ABUSClassifier(device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([8, 1, 500, 500]) torch.Size([8, 1, 500, 500]) torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataloader:\n",
    "    print(len(data))\n",
    "    x,y,l = data\n",
    "    y = y.unsqueeze(1)\n",
    "    l = l.unsqueeze(1)\n",
    "    print(x.shape, y.shape, l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, loss_fun_seg, loss_fun_cls):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"training model...\")\n",
    "    model.train()\n",
    "    loop = tqdm(dataset)\n",
    "    \n",
    "    for data in loop:\n",
    "        x,y,l = data\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.unsqueeze(1).to(device)\n",
    "        l = l.unsqueeze(1).to(device)\n",
    "        x = x - x*y*0.3\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            seg_predictions, cls_predictions = model(x)\n",
    "            seg_predictions = torch.sigmoid(seg_predictions)\n",
    "            cls_loss = loss_fun_cls(cls_predictions, l)\n",
    "            seg_loss = loss_fun_seg(seg_predictions, y)\n",
    "            loss = 0.8*cls_loss + 0.2*seg_loss\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "            \n",
    "        # with torch.no_grad():\n",
    "        #     cls_preds = torch.sigmoid(cls_predictions)\n",
    "        #     cls_preds = (cls_preds > 0.5)\n",
    "        #     num_corrects += (cls_preds == labels).sum()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Empty gpu memory\n",
    "        x = None\n",
    "        y = None\n",
    "        l = None\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Here we have to do the vol classification task\n",
    "        # prediction = model.end_volume().unsqueeze(0)\n",
    "        # label = torch.tensor([l[0]]).to(device).unsqueeze(0)\n",
    "        # vol_cls_loss = loss_fn(prediction, label)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     vol_pred = torch.sigmoid(prediction)\n",
    "        #     vol_pred = (vol_pred > 0.5)\n",
    "        #     num_volume_corrects = (prediction == label).sum()\n",
    "            \n",
    "            # Total loss is calculated as below\n",
    "\n",
    "\n",
    "# train(model, train_dataloader, optimizer, criterion_dice, criterion_bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e854ff437a2467c8fabcbb2bb62abe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozma/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(dataset, model, device=\"cuda\"):\n",
    "    \n",
    "    print(\"calculating model accuracy...\")\n",
    "    num_correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    loop = tqdm(dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loop:\n",
    "            x, y, l = data\n",
    "            \n",
    "            y = y.unsqueeze(1)\n",
    "            l = l.unsqueeze(1)\n",
    "            x = x - x*y*0.3\n",
    "            \n",
    "            x = x.to(device)\n",
    "            l = l.to(device)\n",
    "            \n",
    "            # forward\n",
    "            _, cls_predictions = model(x)\n",
    "            cls_predictions = (torch.sigmoid(cls_predictions) > 0.5).float()\n",
    "            num_correct += (cls_predictions == l).sum()\n",
    "            \n",
    "            x = None\n",
    "            y = None\n",
    "            l = None\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            loop.set_postfix( acc=(num_correct/len(train_dataset)).item())\n",
    "    model.train()\n",
    "    \n",
    "calculate_accuracy(validation_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n"
     ]
    }
   ],
   "source": [
    "def save_results_as_imgs(model, dataset, path=\"./saved_images\"):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Saving the results as images...\")\n",
    "    model.eval()    \n",
    "    for idx, data in enumerate(dataset):\n",
    "        x,y,l = data\n",
    "        y = y.unsqueeze(1)\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            segmentation_preds, _  = model(x)\n",
    "            segmentation_preds = torch.sigmoid(segmentation_preds)\n",
    "            segmentation_preds = (segmentation_preds > 0.5).float()\n",
    "            torchvision.utils.save_image(segmentation_preds, f\"{path}/prediction.png\")\n",
    "            torchvision.utils.save_image(y, f\"{path}/ground_truth.png\")\n",
    "        x = None\n",
    "        torch.cuda.empty_cache()\n",
    "        break\n",
    "    model.train()\n",
    "    \n",
    "save_results_as_imgs(model, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecac9de64544838af084e16be4833e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0a5aa9b33440c38f377d8106d9fb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7346b04e7f40e486f4eaba65b65292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed572689cb8e460cb86e9667b21866cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1333721a3434473cbccc83779da6bd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149e73d863624018898f8fc4aea4e6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0f9932f61440dc84b524d2ba824727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2521dd895ba44228412247f12ef970c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452c77e4d2e849f1b35bfaab1f52f9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdfd46e3dd743b5a5a58be0af07505c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bfabd68e8e498ebe1b71e46fffdad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65f93358dee40c080228bc541b57ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66fb7ceeb934d66af7dfddd2f941ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab3a815209a4c9195110e12d654e740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eca37580bf5417e805c144faf003f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831b54601cb746f1bcaca59f2e393a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342adf1217d249028c2e721cda2dec10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27604045d17746ceb423206dc97d9bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n",
      "training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f30a49c0735427796fd13dd817f95c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating model accuracy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb43effd8e2246ffbd247ce2fa568696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the results as images...\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    train(model, train_dataloader, optimizer, criterion_dice, criterion_bce)\n",
    "    calculate_accuracy(validation_dataloader, model)\n",
    "    save_results_as_imgs(model, validation_dataloader)\n",
    "torch.save(model.state_dict(), \"./checkpoint/model.state.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./checkpoint/model.state.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

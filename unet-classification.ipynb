{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import datasets\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import unet\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=256, width=256),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0],\n",
    "                std=[1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=256, width=256),\n",
    "        A.Normalize(\n",
    "            mean=[0.0],\n",
    "            std=[1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.tdsc.TDSC(path_to_dataset=\"./data/tdsc/train\", transform=train_transforms)\n",
    "validation_dataset = datasets.tdsc.TDSC(path_to_dataset=\"./data/tdsc/validation\", transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.UNet(in_channels=1, out_channels=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Flatten predictions and targets\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice_coefficient = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "\n",
    "        # The Dice Loss is the complement of the Dice Coefficient\n",
    "        dice_loss = 1 - dice_coefficient\n",
    "\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_epochs = 20\n",
    "criterion = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = model.to(device=device)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, loss_fn, brighter=0):\n",
    "\n",
    "    model.train()\n",
    "    loop = tqdm(dataset)\n",
    "\n",
    "    for idx, data in enumerate(loop):\n",
    "        vol, mask, y = data\n",
    "        for i in range(vol.shape[1]):\n",
    "            x = vol[:,i,:,:]\n",
    "            m = mask[:,i,:,:]\n",
    "            if brighter != 0:\n",
    "                x = x + m*brighter\n",
    "            x = x.unsqueeze(0)\n",
    "            m = m.unsqueeze(0)\n",
    "            x = x.to(device)\n",
    "            m = m.to(device)\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = torch.sigmoid(model(x))\n",
    "                loss = loss_fn(predictions, m)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(dataset, model, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vol, mask, l in dataset:\n",
    "            for i in range(vol.shape[1]):\n",
    "                x = vol[:,i,:,:]\n",
    "                y = mask[:,i,:,:]\n",
    "                x = x.unsqueeze(0)\n",
    "                y = y.unsqueeze(0)\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).unsqueeze(1)\n",
    "                preds = torch.sigmoid(model(x))\n",
    "                preds = (preds > 0.5).float()\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_pixels += torch.numel(preds)\n",
    "                dice_score += (2 * (preds * y).sum()) / (\n",
    "                    (preds + y).sum() + 1e-8\n",
    "                )\n",
    "\n",
    "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(dataset)}\")\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 256, 256]) torch.Size([37, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x,m,y = train_dataset[10]\n",
    "x = x.squeeze(0)\n",
    "m = m.squeeze(0)\n",
    "print(x.shape, m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = x[10,:,:]\n",
    "sm = y[10,:,:]\n",
    "\n",
    "print(sx.shape, sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_as_imgs(model, dataset, path=\"./saved_images\"):\n",
    "    model.eval()\n",
    "    idx = np.random.randint(0,20)\n",
    "    vol, mask, label = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        for i in range(vol.shape[1]):\n",
    "            x = vol[:,i,:,:]\n",
    "            y = mask[:,i,:,:]\n",
    "            x = x.unsqueeze(0)\n",
    "            y = y.unsqueeze(0)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            y = y.squeeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            torchvision.utils.save_image(preds, f\"{path}/pred_{i}.png\")\n",
    "            torchvision.utils.save_image(y.squeeze(1), f\"{path}/{i}.png\")\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523f15e65d43411ca2926663d1a75826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozma/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9098fdf17adf41a6bf0efa93e558dbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a760f3b19a4ff9867494790d775349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b438c6809148c48922d16d84f63a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dd589bc8604e488b1535ffc1caa72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8555e4798d4b799be2de5d332f6165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d68bb6b288d4610a9233ac12e013bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23579041b024154a42b1c70aed1658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2443d54a2d274752ae943cd7a174073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50649e3c37d54a3fa34a93cc9fd907c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7708721de44f668c1d1670c1ed601b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594e51e5d3554523809719b72df7a80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5101f8c6344b859ba90a2c44a25e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 38786331/39780352 with acc 97.50\n",
      "Dice score: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bc78ca02ba41918e381fae328f0a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ozma/Source/abus-classification/unet-classification.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(model, train_dataset, optimizer, criterion)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     calculate_accuracy(validation_dataset, model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     save_results_as_imgs(model, validation_dataset)\n",
      "\u001b[1;32m/home/ozma/Source/abus-classification/unet-classification.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m vol, mask, l \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vol\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/unet-classification.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             x \u001b[39m=\u001b[39m vol[:,i,:,:]\n",
      "File \u001b[0;32m~/Source/abus-classification/datasets/tdsc.py:198\u001b[0m, in \u001b[0;36mTDSC.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mglobal\u001b[39;00m tumor_locals\n\u001b[1;32m    196\u001b[0m label, path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index]\n\u001b[0;32m--> 198\u001b[0m vol, _ \u001b[39m=\u001b[39m nrrd\u001b[39m.\u001b[39;49mread(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_to_dataset\u001b[39m}\u001b[39;49;00m\u001b[39m/data/\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    199\u001b[0m mask, _ \u001b[39m=\u001b[39m nrrd\u001b[39m.\u001b[39mread(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_to_dataset\u001b[39m}\u001b[39;00m\u001b[39m/mask/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)        \n\u001b[1;32m    200\u001b[0m tumor_local_idx \u001b[39m=\u001b[39m (\u001b[39mint\u001b[39m(path\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.nrrd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/b/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/nrrd/reader.py:517\u001b[0m, in \u001b[0;36mread\u001b[0;34m(filename, custom_field_map, index_order)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fh:\n\u001b[1;32m    516\u001b[0m     header \u001b[39m=\u001b[39m read_header(fh, custom_field_map)\n\u001b[0;32m--> 517\u001b[0m     data \u001b[39m=\u001b[39m read_data(header, fh, filename, index_order)\n\u001b[1;32m    519\u001b[0m \u001b[39mreturn\u001b[39;00m data, header\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/nrrd/reader.py:446\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(header, fh, filename, index_order)\u001b[0m\n\u001b[1;32m    443\u001b[0m end_index \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(start_index \u001b[39m+\u001b[39m _READ_CHUNKSIZE, compressed_data_len)\n\u001b[1;32m    445\u001b[0m \u001b[39m# Decompress and append data\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m decompressed_data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m decompobj\u001b[39m.\u001b[39;49mdecompress(compressed_data[start_index:end_index])\n\u001b[1;32m    448\u001b[0m \u001b[39m# Update start index\u001b[39;00m\n\u001b[1;32m    449\u001b[0m start_index \u001b[39m=\u001b[39m end_index\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    train(model, train_dataset, optimizer, criterion)\n",
    "    calculate_accuracy(validation_dataset, model)\n",
    "    save_results_as_imgs(model, validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "(256, 256) (256, 256)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "x, m, y = validation_dataset[10]\n",
    "prediction = torch.sigmoid(model(x[:,14:15,:,:].to(device)))\n",
    "prediction = (prediction > 0.5).float()\n",
    "print (prediction.shape)\n",
    "pred = prediction.squeeze()\n",
    "m = m[:,14:15,:,:]\n",
    "m = m.squeeze()\n",
    "pred = pred.to('cpu').detach().numpy()\n",
    "m = m.to('cpu').detach().numpy()\n",
    "print (m.shape, pred.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pred)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(m)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation using UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.optim\n",
    "import torchvision\n",
    "from datasets.carvana import Carvana\n",
    "from models.unet import UNet\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from utils import (\n",
    "    load_checkpoint,\n",
    "    save_checkpoint,\n",
    "    get_loaders,\n",
    "    calculate_accuracy,\n",
    "    save_predictions_as_imgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "\n",
    "print ('Using GPU' if torch.cuda.is_available() else \"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building image transformers for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=160, width=240),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader = get_loaders(dataset_path=\"./data/carvana\", batch_size=BATCH_SIZE ,train_transform=train_transforms, val_transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model, optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozma/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7569271/35443200 with acc 21.36\n",
      "Dice score: 0.34863701462745667\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimize_fn = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "calculate_accuracy(validation_loader, model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2907ef7557b54d899080676e50ba8c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 35249310/35443200 with acc 99.45\n",
      "Dice score: 0.9869749546051025\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_images//pred_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ozma/Source/abus-classification/carvana.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/carvana.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m calculate_accuracy(validation_loader, model, device\u001b[39m=\u001b[39mDEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/carvana.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print some examples to a folder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/carvana.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m save_predictions_as_imgs(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/carvana.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     validation_loader, model, folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msaved_images/\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49mDEVICE\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ozma/Source/abus-classification/carvana.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Source/abus-classification/utils.py:46\u001b[0m, in \u001b[0;36msave_predictions_as_imgs\u001b[0;34m(loader, model, folder, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m         preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(model(x))\n\u001b[1;32m     45\u001b[0m         preds \u001b[39m=\u001b[39m (preds \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 46\u001b[0m     torchvision\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49msave_image(\n\u001b[1;32m     47\u001b[0m         preds, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mfolder\u001b[39m}\u001b[39;49;00m\u001b[39m/pred_\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39msave_image(y\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfolder\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    150\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 151\u001b[0m im\u001b[39m.\u001b[39;49msave(fp, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/PIL/Image.py:2428\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2426\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2431\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_images//pred_0.png'"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_fn(train_loader, model, optimize_fn, loss_fn, scaler)\n",
    "\n",
    "    # save model\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\":optimize_fn.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    # check accuracy\n",
    "    calculate_accuracy(validation_loader, model, device=DEVICE)\n",
    "\n",
    "    # print some examples to a folder\n",
    "    save_predictions_as_imgs(\n",
    "        validation_loader, model, folder=\"saved_images/\", device=DEVICE\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
